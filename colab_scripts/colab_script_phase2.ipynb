{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee535d7a",
   "metadata": {},
   "source": [
    "**This is the first script of Phase-2**\n",
    "\n",
    "17/11/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6502a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "# ---------------------\n",
    "# Reproducibility\n",
    "# ---------------------\n",
    "torch.manual_seed(1337)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ---------------------\n",
    "# Config\n",
    "# ---------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    # data\n",
    "    dataset_path: str = \"dataset/new_input.txt\"\n",
    "    tokenizer_dir: str = \"tokenizer\"\n",
    "    tokenizer_file: str = \"tokenizer/bpe_tokenizer.json\"\n",
    "    vocab_size: int = 10000\n",
    "    # model\n",
    "    n_embd: int = 384\n",
    "    n_head: int = 6\n",
    "    n_layer: int = 6\n",
    "    dropout: float = 0.2\n",
    "    block_size: int = 256\n",
    "    # training\n",
    "    batch_size: int = 64\n",
    "    max_iters: int = 25000\n",
    "    eval_interval: int = 250\n",
    "    eval_iters: int = 200\n",
    "    learning_rate: float = 3e-4\n",
    "    min_lr: float = 3e-5\n",
    "    warmup_iters: int = 2000\n",
    "    weight_decay: float = 0.1\n",
    "    betas: tuple = (0.9, 0.95)\n",
    "    grad_clip: float = 1.0\n",
    "    grad_accum_steps: int = 1\n",
    "    use_amp: bool = True\n",
    "    save_every: int = 500\n",
    "    ckpt_dir: str = \"checkpoints\"\n",
    "    from typing import Optional\n",
    "    resume_path: Optional[str] = None\n",
    "    # generation\n",
    "    gen_max_new_tokens: int = 700\n",
    "    gen_temperature: float = 1.0\n",
    "    gen_top_k: int = 50\n",
    "\n",
    "cfg = Config()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ---------------------\n",
    "# Clone dataset repo (ADDED FOR COLAB)\n",
    "# ---------------------\n",
    "if not os.path.exists(cfg.dataset_path):\n",
    "    print(\"üì• Cloning dataset repo...\")\n",
    "    os.system(\"git clone https://github.com/DevashishXO/GPT-From-Scratch.git\")\n",
    "    os.chdir(\"GPT-From-Scratch\")\n",
    "else:\n",
    "    print(\"‚úÖ Dataset already exists.\")\n",
    "\n",
    "# ---------------------\n",
    "# Tokenizer setup\n",
    "# ---------------------\n",
    "os.makedirs(cfg.tokenizer_dir, exist_ok=True)\n",
    "\n",
    "def train_or_load_tokenizer():\n",
    "    tok_path = cfg.tokenizer_file\n",
    "    if not os.path.exists(tok_path):\n",
    "        print(\"üîß Training new BPE tokenizer...\")\n",
    "        tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "        trainer = trainers.BpeTrainer(\n",
    "            vocab_size=cfg.vocab_size,\n",
    "            special_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"]\n",
    "        )\n",
    "        tokenizer.train([cfg.dataset_path], trainer)\n",
    "        if isinstance(tokenizer.model, models.BPE):\n",
    "            tokenizer.model.unk_token = \"[UNK]\"\n",
    "        bos_id = tokenizer.token_to_id(\"[BOS]\")\n",
    "        eos_id = tokenizer.token_to_id(\"[EOS]\")\n",
    "        tokenizer.post_processor = TemplateProcessing(\n",
    "            single=\"[BOS] $A [EOS]\",\n",
    "            pair=\"[BOS] $A [EOS] [BOS] $B [EOS]\",\n",
    "            special_tokens=[(\"[BOS]\", bos_id), (\"[EOS]\", eos_id)]\n",
    "        )\n",
    "        tokenizer.save(tok_path)\n",
    "        print(\"‚úÖ Tokenizer trained and saved.\")\n",
    "    else:\n",
    "        print(\"‚úÖ Existing tokenizer found. Loading...\")\n",
    "    tokenizer = Tokenizer.from_file(tok_path)\n",
    "    if isinstance(tokenizer.model, models.BPE) and tokenizer.model.unk_token is None:\n",
    "        tokenizer.model.unk_token = \"[UNK]\"\n",
    "    bos_id = tokenizer.token_to_id(\"[BOS]\")\n",
    "    eos_id = tokenizer.token_to_id(\"[EOS]\")\n",
    "    if tokenizer.post_processor is None:\n",
    "        tokenizer.post_processor = TemplateProcessing(\n",
    "            single=\"[BOS] $A [EOS]\",\n",
    "            pair=\"[BOS] $A [EOS] [BOS] $B [EOS]\",\n",
    "            special_tokens=[(\"[BOS]\", bos_id), (\"[EOS]\", eos_id)]\n",
    "        )\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = train_or_load_tokenizer()\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f\"üìä Vocab size: {vocab_size}\")\n",
    "\n",
    "BOS_ID = tokenizer.token_to_id(\"[BOS]\")\n",
    "EOS_ID = tokenizer.token_to_id(\"[EOS]\")\n",
    "\n",
    "def encode_text_stream_with_boundaries(txt: str) -> list[int]:\n",
    "    ids: list[int] = []\n",
    "    for line in txt.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        ids.extend(tokenizer.encode(line).ids)\n",
    "    return ids\n",
    "\n",
    "def decode_ids(ids: list[int]) -> str:\n",
    "    return tokenizer.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "# ---------------------\n",
    "# Load dataset\n",
    "# ---------------------\n",
    "with open(cfg.dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(f\"üìñ Dataset length (chars): {len(raw_text):,}\")\n",
    "\n",
    "all_ids = encode_text_stream_with_boundaries(raw_text)\n",
    "data = torch.tensor(all_ids, dtype=torch.long)\n",
    "if len(data) < cfg.block_size + 1:\n",
    "    raise ValueError(f\"Encoded dataset too small ({len(data)}) for block_size={cfg.block_size}\")\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split: str):\n",
    "    src = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, len(src) - cfg.block_size, (cfg.batch_size,))\n",
    "    x = torch.stack([src[i:i+cfg.block_size] for i in ix])\n",
    "    y = torch.stack([src[i+1:i+cfg.block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "# ---------------------\n",
    "# LR schedule\n",
    "# ---------------------\n",
    "def cosine_lr(step: int, base_lr: float, min_lr: float, warmup: int, total: int):\n",
    "    if step < warmup:\n",
    "        return base_lr * step / max(1, warmup)\n",
    "    progress = (step - warmup) / max(1, total - warmup)\n",
    "    return min_lr + 0.5 * (base_lr - min_lr) * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "# ---------------------\n",
    "# Model\n",
    "# ---------------------\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size: int):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(cfg.n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(cfg.n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(cfg.n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(cfg.block_size, cfg.block_size)))\n",
    "        self.dropout = nn.Dropout(cfg.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, head_size: int):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, cfg.n_embd)\n",
    "        self.dropout = nn.Dropout(cfg.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(cfg.dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd: int, n_head: int):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ff = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, cfg.n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(cfg.block_size, cfg.n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(cfg.n_embd, cfg.n_head) for _ in range(cfg.n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(cfg.n_embd)\n",
    "        self.lm_head = nn.Linear(cfg.n_embd, vocab_size, bias=False)\n",
    "        self.apply(self._init_weights)\n",
    "        self.tie_weights()\n",
    "\n",
    "    def tie_weights(self):\n",
    "        self.lm_head.weight = self.token_embedding_table.weight\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(B*T, -1), targets.view(B*T))\n",
    "        return logits, loss\n",
    "\n",
    "    def _sample_next(self, logits_last, temperature=1.0, top_k=50):\n",
    "        if temperature != 1.0:\n",
    "            logits_last = logits_last / temperature\n",
    "        probs = F.softmax(logits_last, dim=-1)\n",
    "        if top_k is not None and top_k > 0:\n",
    "            v, ix = torch.topk(probs, top_k)\n",
    "            mask = torch.ones_like(probs, dtype=torch.bool)\n",
    "            mask.scatter_(1, ix, False)\n",
    "            probs = probs.masked_fill(mask, 0)\n",
    "            probs = probs / probs.sum(dim=-1, keepdim=True)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        return next_token\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=50, eos_id: int | None = None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -cfg.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            next_token = self._sample_next(logits[:, -1, :], temperature, top_k)\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "            if eos_id is not None and (next_token == eos_id).all():\n",
    "                break\n",
    "        return idx\n",
    "\n",
    "# ---------------------\n",
    "# Init model/opt/amp\n",
    "# ---------------------\n",
    "model = GPTLanguageModel().to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=cfg.learning_rate,\n",
    "    weight_decay=cfg.weight_decay,\n",
    "    betas=cfg.betas\n",
    ")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(cfg.use_amp and device == 'cuda'))\n",
    "\n",
    "param_millions = sum(p.numel() for p in model.parameters())/1e6\n",
    "print(f\"ü§ñ Model parameters: {param_millions:.2f} M\")\n",
    "\n",
    "os.makedirs(cfg.ckpt_dir, exist_ok=True)\n",
    "\n",
    "start_iter = 0\n",
    "best_val_loss = float('inf')\n",
    "if cfg.resume_path is not None and os.path.exists(cfg.resume_path):\n",
    "    ckpt = torch.load(cfg.resume_path, map_location=device)\n",
    "    model.load_state_dict(ckpt['model'])\n",
    "    optimizer.load_state_dict(ckpt['opt'])\n",
    "    start_iter = ckpt.get('iter', 0) + 1\n",
    "    best_val_loss = ckpt.get('val_loss', float('inf'))\n",
    "    print(f\"üîÑ Resumed from {cfg.resume_path} at iter {start_iter}, best_val_loss={best_val_loss:.4f}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_metrics():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(cfg.eval_iters, device=device)\n",
    "        for k in range(cfg.eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with torch.cuda.amp.autocast(enabled=(cfg.use_amp and device == 'cuda')):\n",
    "                _, loss = model(X, Y)\n",
    "            losses[k] = loss\n",
    "        mean_loss = losses.mean().item()\n",
    "        out[split] = {'loss': mean_loss, 'ppl': math.exp(mean_loss)}\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# ---------------------\n",
    "# Training\n",
    "# ---------------------\n",
    "for iter in range(start_iter, cfg.max_iters):\n",
    "    lr = cosine_lr(iter, cfg.learning_rate, cfg.min_lr, cfg.warmup_iters, cfg.max_iters)\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = lr\n",
    "\n",
    "    if iter % 100 == 0 and iter > 0:\n",
    "        print(f\"[Progress: {iter}/{cfg.max_iters} ({100*iter/cfg.max_iters:.1f}%)]\")\n",
    "\n",
    "    if iter % cfg.eval_interval == 0 or iter == cfg.max_iters - 1:\n",
    "        metrics = estimate_metrics()\n",
    "        print(f\"Step {iter}: train loss {metrics['train']['loss']:.4f} (ppl {metrics['train']['ppl']:.2f}), \"\n",
    "              f\"val loss {metrics['val']['loss']:.4f} (ppl {metrics['val']['ppl']:.2f}), lr {lr:.2e}\")\n",
    "        if metrics['val']['loss'] < best_val_loss:\n",
    "            best_val_loss = metrics['val']['loss']\n",
    "            best_path = os.path.join(cfg.ckpt_dir, \"best.pt\")\n",
    "            torch.save({'iter': iter, 'model': model.state_dict(), 'opt': optimizer.state_dict(),\n",
    "                        'val_loss': best_val_loss, 'cfg': cfg.__dict__}, best_path)\n",
    "            print(f\" -> üíæ Best checkpoint updated: {best_path}\")\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    for micro in range(cfg.grad_accum_steps):\n",
    "        xb, yb = get_batch('train')\n",
    "        with torch.cuda.amp.autocast(enabled=(cfg.use_amp and device == 'cuda')):\n",
    "            _, loss = model(xb, yb)\n",
    "            loss = loss / cfg.grad_accum_steps\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    if cfg.grad_clip is not None and cfg.grad_clip > 0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    if iter > 0 and iter % cfg.save_every == 0:\n",
    "        ckpt_path = os.path.join(cfg.ckpt_dir, f\"checkpoint_{iter}.pt\")\n",
    "        torch.save({'iter': iter, 'model': model.state_dict(), 'opt': optimizer.state_dict(),\n",
    "                    'val_loss': best_val_loss, 'cfg': cfg.__dict__}, ckpt_path)\n",
    "        print(f\"üíæ Checkpoint saved at step {iter}: {ckpt_path}\")\n",
    "    if iter % 100 == 0:  # print every 100 steps\n",
    "        print(f\"[{iter}/{cfg.max_iters}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ---------------------\n",
    "# Save final, generate sample\n",
    "# ---------------------\n",
    "final_model_path = os.path.join(cfg.ckpt_dir, \"gpt_final.pt\")\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"‚úÖ Final model saved to {final_model_path}\")\n",
    "\n",
    "context = torch.tensor([[BOS_ID]], dtype=torch.long, device=device)\n",
    "generated_ids = model.generate(\n",
    "    context,\n",
    "    max_new_tokens=cfg.gen_max_new_tokens,\n",
    "    temperature=cfg.gen_temperature,\n",
    "    top_k=cfg.gen_top_k,\n",
    "    eos_id=EOS_ID\n",
    ")[0].tolist()\n",
    "print(\"üìù Sample:\\n\", decode_ids(generated_ids))\n",
    "\n",
    "# ---------------------\n",
    "# Zip checkpoints for download (ADDED FOR COLAB)\n",
    "# ---------------------\n",
    "print(\"\\nüì¶ Zipping checkpoints for download...\")\n",
    "os.system(f\"zip -r checkpoints.zip {cfg.ckpt_dir}\")\n",
    "print(\"‚úÖ Run this in a new cell to download:\")\n",
    "print(\"   from google.colab import files\")\n",
    "print(\"   files.download('checkpoints.zip')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf327bfc",
   "metadata": {},
   "source": [
    "### **Output of the above script**\n",
    "\n",
    "Below is the output produced by the script:\n",
    "\n",
    "![Output of the script (phase2_script1.png)](phase2_script1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d675a378",
   "metadata": {},
   "source": [
    "**Text Generated:**\n",
    "\n",
    " ‡§§‡§æ‡§π‡§ø‡§∞ - ‡§∏‡§¨ ‡§π‡•Å‡§ú‡•Ç‡§∞ , ‡§Ö‡§ú‡§æ‡§¨ ‡§¨‡§°‡§º‡•á ‡§ó‡•Å‡§®‡§æ‡§π ‡§∏‡•á ‡§®‡§π‡•Ä‡§Ç ‡§®‡§ø‡§ï‡§≤‡§§‡§æ ‡•§ ‡§Æ‡•à‡§Ç ‡§Ü‡§ú‡§ï‡§≤ ‡§è‡§ï ‡§Ö‡§∞‡•ç‡§ú ‡§ï‡§∞‡•Ç‡§Å‡§ó‡§æ ‡§ï‡§ø ‡§ï‡§≤ ‡§≠‡•Ä ‡§§‡§æ‡§°‡§º‡•Ä ‡§ï‡•á ‡§¶‡•Ç‡§ß ‡§™‡§∞ ‡§∞‡•ã , ‡§ö‡§æ‡§π‡•á , ‡§ö‡§æ‡§π‡•á ‡§µ‡§π ‡§¨‡§°‡§º‡§æ ‡§Ö‡§®‡•Å‡§∑‡•ç‡§†‡§æ‡§® ‡§ï‡§∞‡•á , ‡§™‡§∞ ‡§Æ‡•á‡§∞‡•Ä ‡§§‡•ã ‡§ú‡§æ‡§® ‡§π‡•Ä ‡§ï‡•ç‡§Ø‡•ã‡§Ç ‡§® ‡§≤‡•á‡§®‡•Ä ‡§™‡§°‡§º‡•á ‡•§ ‡§Ö‡§¨ ‡§®‡§π‡•Ä‡§Ç ‡§™‡§°‡§º‡§§‡§æ ‡§ï‡§ø ‡§Æ‡•á‡§∞‡•á ‡§™‡•Ä‡§õ‡•á ‡§∏‡•á ‡§∞‡•ã - ‡§∞‡•ã ‡§™‡§°‡§º‡•á ‡§π‡•Å‡§è ‡§π‡•à‡§Ç ‡•§ ‡§ú‡§∞‡§æ ‡§â‡§®‡§∏‡•á ‡§∞‡•Å‡§™‡§Ø‡•á - ‡§™‡•à‡§∏‡•á ‡§ï‡•Ä ‡§ú‡§∞‡•Ç‡§∞‡§§ ‡§®‡§π‡•Ä‡§Ç ‡•§ '\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fa7da5",
   "metadata": {},
   "source": [
    "**What went wrong?**\n",
    "\n",
    "Train loss dropped to ~0.94, but val loss plateaued ~6.4 (exp ‚âà 630). This suggests overfitting or tokenization/corpus difficulty.\n",
    "\n",
    "**What about the checkpoints and the tokenizer?**\n",
    "\n",
    "Colab free runtimes are ephemeral. When the runtime disconnects, the whole /content filesystem is reset. The zip file existed before disconnect, but vanished after the restart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a127a7ae",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1262e1fd",
   "metadata": {},
   "source": [
    "## **This is the second script of Phase-2**\n",
    "\n",
    "18/11/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72f4fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eacca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "# ---------------------\n",
    "# Reproducibility\n",
    "# ---------------------\n",
    "torch.manual_seed(1337)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ---------------------\n",
    "# Config\n",
    "# ---------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    # data\n",
    "    dataset_path: str = \"dataset/new_input.txt\"\n",
    "    tokenizer_dir: str = \"tokenizer\"\n",
    "    tokenizer_file: str = \"tokenizer/bpe_tokenizer.json\"\n",
    "    vocab_size: int = 8000 # changed from 10000\n",
    "    force_retrain_tokenizer: bool = False  # set True to retrain with ByteLevel BPE\n",
    "    # model\n",
    "    n_embd: int = 384\n",
    "    n_head: int = 6\n",
    "    n_layer: int = 4 # changed from 6\n",
    "    dropout: float = 0.4           # stronger regularization\n",
    "    emb_dropout: float = 0.1       # new: embedding dropout\n",
    "    block_size: int = 256\n",
    "    # training\n",
    "    batch_size: int = 32 # changed from 64\n",
    "    max_iters: int = 25000\n",
    "    eval_interval: int = 1000 # changed from 250\n",
    "    eval_iters: int = 50 # changed from 200\n",
    "    learning_rate: float = 3e-4\n",
    "    min_lr: float = 3e-5\n",
    "    warmup_iters: int = 2000\n",
    "    weight_decay: float = 0.1\n",
    "    betas: tuple = (0.9, 0.95)\n",
    "    grad_clip: float = 1.0\n",
    "    grad_accum_steps: int = 2 # changed from 1\n",
    "    use_amp: bool = True\n",
    "    patience_evals: int = 6        # new: early stopping patience\n",
    "    save_every: int = 500\n",
    "    ckpt_dir: str = \"checkpoints\"\n",
    "    resume_path: Optional[str] = None\n",
    "    # loss\n",
    "    label_smoothing: float = 0.1   # new: improves generalization\n",
    "    # generation\n",
    "    gen_max_new_tokens: int = 700\n",
    "    gen_temperature: float = 1.0\n",
    "    gen_top_k: int = 50\n",
    "\n",
    "cfg = Config()\n",
    "# === [NEW] Set up Drive output directories ===\n",
    "import datetime\n",
    "run_id = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "drive_root = f\"/content/drive/MyDrive/gpt_from_scratch_runs/run_{run_id}\"\n",
    "\n",
    "cfg.ckpt_dir = os.path.join(drive_root, \"checkpoints\")\n",
    "cfg.tokenizer_dir = os.path.join(drive_root, \"tokenizer\")\n",
    "cfg.tokenizer_file = os.path.join(cfg.tokenizer_dir, \"bpe_tokenizer.json\")\n",
    "os.makedirs(cfg.ckpt_dir, exist_ok=True)\n",
    "os.makedirs(cfg.tokenizer_dir, exist_ok=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ---------------------\n",
    "# Clone dataset repo (COLAB)\n",
    "# ---------------------\n",
    "if not os.path.exists(cfg.dataset_path):\n",
    "    print(\"üì• Cloning dataset repo...\")\n",
    "    os.system(\"git clone https://github.com/DevashishXO/GPT-From-Scratch.git\")\n",
    "    os.chdir(\"GPT-From-Scratch\")\n",
    "else:\n",
    "    print(\"‚úÖ Dataset already exists.\")\n",
    "\n",
    "# ---------------------\n",
    "# Tokenizer setup\n",
    "# ---------------------\n",
    "os.makedirs(cfg.tokenizer_dir, exist_ok=True)\n",
    "\n",
    "def train_or_load_tokenizer():\n",
    "    tok_path = cfg.tokenizer_file\n",
    "    if cfg.force_retrain_tokenizer and os.path.exists(tok_path):\n",
    "        print(\"‚ôªÔ∏è Forcing tokenizer retrain, removing old file...\")\n",
    "        try:\n",
    "            os.remove(tok_path)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    if not os.path.exists(tok_path):\n",
    "        print(\"üîß Training new BPE tokenizer...\")\n",
    "        tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "        # ByteLevel is more robust for multilingual/raw punctuation; switch if retraining\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel() if cfg.force_retrain_tokenizer else pre_tokenizers.Whitespace()\n",
    "        trainer = trainers.BpeTrainer(\n",
    "            vocab_size=cfg.vocab_size,\n",
    "            special_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"]\n",
    "        )\n",
    "        tokenizer.train([cfg.dataset_path], trainer)\n",
    "        if isinstance(tokenizer.model, models.BPE):\n",
    "            tokenizer.model.unk_token = \"[UNK]\"\n",
    "        bos_id = tokenizer.token_to_id(\"[BOS]\")\n",
    "        eos_id = tokenizer.token_to_id(\"[EOS]\")\n",
    "        tokenizer.post_processor = TemplateProcessing(\n",
    "            single=\"[BOS] $A [EOS]\",\n",
    "            pair=\"[BOS] $A [EOS] [BOS] $B [EOS]\",\n",
    "            special_tokens=[(\"[BOS]\", bos_id), (\"[EOS]\", eos_id)]\n",
    "        )\n",
    "        tokenizer.save(tok_path)\n",
    "        print(\"‚úÖ Tokenizer trained and saved.\")\n",
    "    else:\n",
    "        print(\"‚úÖ Existing tokenizer found. Loading...\")\n",
    "    tokenizer = Tokenizer.from_file(tok_path)\n",
    "    if isinstance(tokenizer.model, models.BPE) and tokenizer.model.unk_token is None:\n",
    "        tokenizer.model.unk_token = \"[UNK]\"\n",
    "    bos_id = tokenizer.token_to_id(\"[BOS]\")\n",
    "    eos_id = tokenizer.token_to_id(\"[EOS]\")\n",
    "    if tokenizer.post_processor is None:\n",
    "        tokenizer.post_processor = TemplateProcessing(\n",
    "            single=\"[BOS] $A [EOS]\",\n",
    "            pair=\"[BOS] $A [EOS] [BOS] $B [EOS]\",\n",
    "            special_tokens=[(\"[BOS]\", bos_id), (\"[EOS]\", eos_id)]\n",
    "        )\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = train_or_load_tokenizer()\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f\"üìä Vocab size: {vocab_size}\")\n",
    "\n",
    "BOS_ID = tokenizer.token_to_id(\"[BOS]\")\n",
    "EOS_ID = tokenizer.token_to_id(\"[EOS]\")\n",
    "\n",
    "def encode_text_stream_with_boundaries(txt: str) -> list[int]:\n",
    "    ids: list[int] = []\n",
    "    for line in txt.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        ids.extend(tokenizer.encode(line).ids)  # post-processor injects BOS/EOS\n",
    "    return ids\n",
    "\n",
    "def decode_ids(ids: list[int]) -> str:\n",
    "    return tokenizer.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "# ---------------------\n",
    "# Load dataset\n",
    "# ---------------------\n",
    "with open(cfg.dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(f\"üìñ Dataset length (chars): {len(raw_text):,}\")\n",
    "\n",
    "all_ids = encode_text_stream_with_boundaries(raw_text)\n",
    "data = torch.tensor(all_ids, dtype=torch.long)\n",
    "if len(data) < cfg.block_size + 1:\n",
    "    raise ValueError(f\"Encoded dataset too small ({len(data)}) for block_size={cfg.block_size}\")\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split: str):\n",
    "    src = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, len(src) - cfg.block_size, (cfg.batch_size,))\n",
    "    x = torch.stack([src[i:i+cfg.block_size] for i in ix])\n",
    "    y = torch.stack([src[i+1:i+cfg.block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "# ---------------------\n",
    "# LR schedule\n",
    "# ---------------------\n",
    "def cosine_lr(step: int, base_lr: float, min_lr: float, warmup: int, total: int):\n",
    "    if step < warmup:\n",
    "        return base_lr * step / max(1, warmup)\n",
    "    progress = (step - warmup) / max(1, total - warmup)\n",
    "    return min_lr + 0.5 * (base_lr - min_lr) * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "# ---------------------\n",
    "# Model\n",
    "# ---------------------\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size: int):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(cfg.n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(cfg.n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(cfg.n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(cfg.block_size, cfg.block_size)))\n",
    "        self.dropout = nn.Dropout(cfg.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x); q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5)\n",
    "        # AMP-stable mask\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, torch.finfo(wei.dtype).min)\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, head_size: int):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, cfg.n_embd)\n",
    "        self.dropout = nn.Dropout(cfg.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),  # GELU generally improves GPTs\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(cfg.dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd: int, n_head: int):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ff = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, cfg.n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(cfg.block_size, cfg.n_embd)\n",
    "        self.drop_emb = nn.Dropout(cfg.emb_dropout)  # new: embedding dropout\n",
    "        self.blocks = nn.Sequential(*[Block(cfg.n_embd, cfg.n_head) for _ in range(cfg.n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(cfg.n_embd)\n",
    "        self.lm_head = nn.Linear(cfg.n_embd, vocab_size, bias=False)\n",
    "        self.apply(self._init_weights)\n",
    "        self.tie_weights()\n",
    "\n",
    "    def tie_weights(self):\n",
    "        self.lm_head.weight = self.token_embedding_table.weight\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
    "        x = self.drop_emb(tok_emb + pos_emb)  # apply embedding dropout\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(B*T, -1),\n",
    "                targets.view(B*T),\n",
    "                label_smoothing=cfg.label_smoothing,  # new: label smoothing\n",
    "            )\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _sample_next(self, logits_last, temperature=1.0, top_k=50):\n",
    "        if temperature != 1.0:\n",
    "            logits_last = logits_last / temperature\n",
    "        probs = F.softmax(logits_last, dim=-1)\n",
    "        if top_k is not None and top_k > 0:\n",
    "            v, ix = torch.topk(probs, top_k)\n",
    "            mask = torch.ones_like(probs, dtype=torch.bool)\n",
    "            mask.scatter_(1, ix, False)\n",
    "            probs = probs.masked_fill(mask, 0)\n",
    "            probs = probs / probs.sum(dim=-1, keepdim=True)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        return next_token\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=50, eos_id: int | None = None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -cfg.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            next_token = self._sample_next(logits[:, -1, :], temperature, top_k)\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "            if eos_id is not None and (next_token == eos_id).all():\n",
    "                break\n",
    "        return idx\n",
    "\n",
    "# ---------------------\n",
    "# Init model/opt/amp\n",
    "# ---------------------\n",
    "model = GPTLanguageModel().to(device)\n",
    "\n",
    "# AdamW param groups: no weight decay on LayerNorm or bias\n",
    "decay = []\n",
    "no_decay = []\n",
    "for name, param in model.named_parameters():\n",
    "    if name.endswith('bias') or 'ln' in name or 'norm' in name:\n",
    "        no_decay.append(param)\n",
    "    else:\n",
    "        decay.append(param)\n",
    "\n",
    "optim_groups = [\n",
    "    {\"params\": decay, \"weight_decay\": cfg.weight_decay},\n",
    "    {\"params\": no_decay, \"weight_decay\": 0.0},\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    optim_groups,\n",
    "    lr=cfg.learning_rate,\n",
    "    betas=cfg.betas\n",
    ")\n",
    "\n",
    "# Use new torch.amp API (fixes deprecation warnings)\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=(cfg.use_amp and device == 'cuda'))\n",
    "\n",
    "param_millions = sum(p.numel() for p in model.parameters())/1e6\n",
    "print(f\"ü§ñ Model parameters: {param_millions:.2f} M\")\n",
    "\n",
    "os.makedirs(cfg.ckpt_dir, exist_ok=True)\n",
    "\n",
    "start_iter = 0\n",
    "best_val_loss = float('inf')\n",
    "no_improve_evals = 0\n",
    "if cfg.resume_path is not None and os.path.exists(cfg.resume_path):\n",
    "    ckpt = torch.load(cfg.resume_path, map_location=device)\n",
    "    model.load_state_dict(ckpt['model'])\n",
    "    optimizer.load_state_dict(ckpt['opt'])\n",
    "    start_iter = ckpt.get('iter', 0) + 1\n",
    "    best_val_loss = ckpt.get('val_loss', float('inf'))\n",
    "    print(f\"üîÑ Resumed from {cfg.resume_path} at iter {start_iter}, best_val_loss={best_val_loss:.4f}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_metrics():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(cfg.eval_iters, device=device)\n",
    "        for k in range(cfg.eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with torch.amp.autocast('cuda', enabled=(cfg.use_amp and device == 'cuda')):\n",
    "                _, loss = model(X, Y)\n",
    "            losses[k] = loss\n",
    "        mean_loss = losses.mean().item()\n",
    "        out[split] = {'loss': mean_loss, 'ppl': math.exp(mean_loss)}\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# ---------------------\n",
    "# Training\n",
    "# ---------------------\n",
    "t0 = time.time()\n",
    "for iter in range(start_iter, cfg.max_iters):\n",
    "    lr = cosine_lr(iter, cfg.learning_rate, cfg.min_lr, cfg.warmup_iters, cfg.max_iters)\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = lr\n",
    "\n",
    "    if iter % 100 == 0 and iter > 0:\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"[Progress: {iter}/{cfg.max_iters} ({100*iter/cfg.max_iters:.1f}%)] elapsed {elapsed/60:.1f} min\")\n",
    "\n",
    "    if iter % cfg.eval_interval == 0 or iter == cfg.max_iters - 1:\n",
    "        metrics = estimate_metrics()\n",
    "        print(f\"Step {iter}: train loss {metrics['train']['loss']:.4f} (ppl {metrics['train']['ppl']:.2f}), \"\n",
    "              f\"val loss {metrics['val']['loss']:.4f} (ppl {metrics['val']['ppl']:.2f}), lr {lr:.2e}\")\n",
    "        if metrics['val']['loss'] + 1e-6 < best_val_loss:\n",
    "            best_val_loss = metrics['val']['loss']\n",
    "            no_improve_evals = 0\n",
    "            best_path = os.path.join(cfg.ckpt_dir, \"best.pt\")\n",
    "            torch.save({'iter': iter, 'model': model.state_dict(), 'opt': optimizer.state_dict(),\n",
    "                        'val_loss': best_val_loss, 'cfg': cfg.__dict__}, best_path)\n",
    "            print(f\" -> üíæ Best checkpoint updated: {best_path}\")\n",
    "        else:\n",
    "            no_improve_evals += 1\n",
    "            if no_improve_evals >= cfg.patience_evals:\n",
    "                print(f\"‚èπÔ∏è Early stopping after {no_improve_evals} evals without val improvement.\")\n",
    "                break\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    for micro in range(cfg.grad_accum_steps):\n",
    "        xb, yb = get_batch('train')\n",
    "        with torch.amp.autocast('cuda', enabled=(cfg.use_amp and device == 'cuda')):\n",
    "            _, loss = model(xb, yb)\n",
    "            loss = loss / cfg.grad_accum_steps\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    if cfg.grad_clip and cfg.grad_clip > 0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    if iter > 0 and iter % cfg.save_every == 0:\n",
    "        ckpt_path = os.path.join(cfg.ckpt_dir, f\"checkpoint_{iter}.pt\")\n",
    "        torch.save({'iter': iter, 'model': model.state_dict(), 'opt': optimizer.state_dict(),\n",
    "                    'val_loss': best_val_loss, 'cfg': cfg.__dict__}, ckpt_path)\n",
    "        print(f\"üíæ Checkpoint saved at step {iter}: {ckpt_path}\")\n",
    "\n",
    "# ---------------------\n",
    "# Save final, generate sample\n",
    "# ---------------------\n",
    "final_model_path = os.path.join(cfg.ckpt_dir, \"gpt_final.pt\")\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"‚úÖ Final model saved to {final_model_path}\")\n",
    "\n",
    "context = torch.tensor([[BOS_ID]], dtype=torch.long, device=device)\n",
    "generated_ids = model.generate(\n",
    "    context,\n",
    "    max_new_tokens=cfg.gen_max_new_tokens,\n",
    "    temperature=cfg.gen_temperature,\n",
    "    top_k=cfg.gen_top_k,\n",
    "    eos_id=EOS_ID\n",
    ")[0].tolist()\n",
    "print(\"üìù Sample:\\n\", decode_ids(generated_ids))\n",
    "\n",
    "# ---------------------\n",
    "# Zip checkpoints for download (COLAB)\n",
    "# ---------------------\n",
    "print(\"\\nüì¶ Zipping checkpoints for download...\")\n",
    "os.system(f\"zip -r checkpoints.zip {cfg.ckpt_dir}\")\n",
    "print(\"‚úÖ Run this in a new cell to download:\")\n",
    "print(\"   from google.colab import files\")\n",
    "print(\"   files.download('checkpoints.zip')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bdb9aa",
   "metadata": {},
   "source": [
    "### **Output of the above script**\n",
    "\n",
    "Below is the output produced by the script:\n",
    "\n",
    "![Output of the script (phase2_script2.png)](phase2_script2.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
